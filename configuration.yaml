# General Configuration
model:
  name: "gazemoe_dinov2_vitl14_inout"  # "gazelle_dinov2_vitl14_inout" OR "gazemoe_dinov2_vitl14_inout"
  pretrained_path: "pretrainShared_epoch_14.pt"  # "gazelle_dinov2_vitl14_inout.pt"
  num_experts: 4  # Routed experts
  num_shared_experts: 2  # Shared experts
  top_k: 2
  moe_type: "shared"  # vanilla / shared / else
  is_msf: 1
  bce_weight: 1.0
  mse_weight: 180.0
  angle_weight: 0.0
  vec_weight: 0.0
  pbce_loss: "bce"  #"mse" or "bce"
  reduction: "mean"  #"mean" or "none"
  encoder:
    type: "DINOv2"                  # Pretrained image encoder (DINOv2, CLIP, etc.)
    pretrained: true                # Use pretrained weights
  decoder:
    hidden_size: 256                # d_model
    depth: 3                        # Number of transformer layers in the decoder (default 3)
    num_heads: 8                    # Number of attention heads
    dropout: 0.1                    # Dropout rate

data:
  input_resolution: 448
  train_path: "./VAT"     # Path to preprocessed training dataset
  test_path: "./VAT"       # Path to preprocessed test dataset
  pre_train_path: "./gazefollow_extended"
  pre_test_path: "./gazefollow_extended"
  augmentations: []

train:
  # VAT finetuning hyperparams
  batch_size: 36                    # Batch size for training
  epochs: 10                    # Number of training epochs
  lr: 0.00001                         # 0.00001
  inout_lr: 0.001
  fuse_lr: 0.00001
  block_lr: 0.00001
  layer_decay: 0                    # 0 or 1
  optimizer: "Adam"                # Optimizer type (adam, adamw, sgd, etc.)
  weight_decay: 0.01                # Weight decay for optimizer
  lr_scheduler:                     # Learning rate scheduler settings
    type: "cosine"                  # cosine / linear / warmup (for warmup training)
    step_size: 15                   # Step size for step scheduler / warmup: warmup_epochs
    gamma: 0.1                      # Decay factor for step/exponential scheduler
    min_lr: 1e-7                    # Minimum learning rate for cosine/other schedulers
  gradient_clipping: 5.0            # Gradient clipping threshold
  # GazeFollow pretrain hyperparams
  pre_lr: 0.001
  pre_fuse_lr: 0.001
  pre_block_lr: 0.001
  pre_batch_size: 60   # 60
  pre_epochs: 15   # 15
  pre_optimizer: "Adam"
  pre_lr_scheduler:
    type: "cosine"                  # cosine / linear
    step_size: 15                   # Step size for step scheduler
    gamma: 0.1                      # Decay factor for step/exponential scheduler
    min_lr: 1e-7                    # Minimum learning rate for cosine/other schedulers

eval:
  checkpoint: ""                    # Path to the checkpoint for Eval
  batch_size: 36                    # Batch size for evaluation

inference:
  checkpoint: ""

logging:
  log_dir: "results/GazeMoE/"           # auto naming
  pre_dir: "results/pretrainShared/"
  save_every: 5

hardware:
  device: "cuda:0"                    # Device to use (cuda, cpu)
  num_workers: 3                    # Number of data loader workers
  pin_memory: True                  # Pin memory for faster data loading
