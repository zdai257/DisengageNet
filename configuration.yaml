# General Configuration
model:
  name: "gazelle_dinov2_vitl14_inout"
  pretrained_path: "best_shared_epoch_14.pt"
  pbce_loss: "bce"  #"mse" or "bce" or "hybrid"
  bce_weight: 1.0
  mse_weight: 0.0
  reduction: "mean"  #"mean" or "none"
  kld_weight: 0.0  # 0.01, 0.02, 0.05, 0.1 [0.0, 0.02, 0.05, 0.1]
  is_focal_loss: 1  # if using FocalLoss for class balancing
  encoder:
    type: "DINOv2"                  # backbone: DINOv2, ViT-B/16, ViT-L/14
    pretrained: true                # Use pretrained weights
    decoder:
      hidden_size: 256                # d_model [256, 384]
      depth: 3                        # Number of transformer layers in the decoder (default 3) [3, 4]
      num_heads: 8                    # Number of attention heads
      dropout: 0.1                    # Dropout rate

data:
  input_resolution: 448
  train_path: "./VAT"     # Path to preprocessed training dataset
  test_path: "./VAT"       # Path to preprocessed test dataset
  mean: [0.485, 0.456, 0.406]       # Normalization mean for images (ImageNet values)
  std: [0.229, 0.224, 0.225]        # Normalization std for images (ImageNet values)
  pre_train_path: "./gazefollow_extended"
  pre_test_path: "./gazefollow_extended"
  augmentations: ["crop", "photometric"]

train:
  batch_size: 48                    # Batch size for training
  epochs: 15                    # Number of training epochs
  lr: 0.00001                         # 0.00001
  inout_lr: 0.001
  fuse_lr: 0.00001
  block_lr: 0.00001
  optimizer: "AdamW"                # Optimizer type (adam, adamw, sgd, etc.)
  weight_decay: 0.01                # Weight decay for optimizer
  lr_scheduler:                     # Learning rate scheduler settings
    type: "cosine"                  # cosine / linear / warmup (for warmup training)
    step_size: 15                   # Step size for step scheduler / warmup: warmup_epochs
    gamma: 0.1                      # Decay factor for step/exponential scheduler
    min_lr: 1e-7                    # Minimum learning rate for cosine/other schedulers
  gradient_clipping: 5.0            # Gradient clipping threshold
  # GazeFollow pretrain params
  pre_lr: 0.001
  pre_batch_size: 60   # 60
  pre_epochs: 15   # 15
  pre_optimizer: "Adam"
  pre_lr_scheduler:
    type: "cosine"                  # cosine / linear
    step_size: 20                   # Step size for step scheduler
    gamma: 0.1                      # Decay factor for step/exponential scheduler
    min_lr: 1e-7                    # Minimum learning rate for cosine/other schedulers

eval:
  checkpoint: "gazelle_large_vitl14_inout.pt"  # Path to the checkpoint for evaluation
  batch_size: 48                    # Batch size for evaluation

logging:
  log_dir: "results/GT360_vat/"
  pre_dir: "results/GT360_pretrain/"
  save_every: 5

hardware:
  device: "cuda:0"                    # Device to use (cuda, cpu)
  num_workers: 3                    # Number of data loader workers
  pin_memory: True                  # Pin memory for faster data loading
