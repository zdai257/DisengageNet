# General Configuration
model:
  name: "gazelle_dinov2_vitl14_inout"
  pretrained_path: "gazelle_dinov2_vitl14_inout.pt"  #"best_shared_epoch_4.pt"
  bce_weight: 1.0
  mse_weight: 1.0
  encoder:
    type: "DINOv2"                  # Pretrained image encoder (DINOv2, CLIP, etc.)
    pretrained: true                # Use pretrained weights
  decoder:
    type: "ViT"                     # Decoder type (ViT, transformer-like)
    hidden_size: 512                # Hidden size of the decoder
    depth: 12                       # Number of layers in the decoder
    num_heads: 8                    # Number of attention heads
    mlp_ratio: 4.0                  # Ratio for MLP in ViT decoder
    dropout: 0.1                    # Dropout rate
    activation: "gelu"              # Activation function (relu, gelu, leaky_relu)
    output_size: 128                # Dimension of decoder output features

data:
  input_resolution: 448
  train_path: "./VAT"     # Path to preprocessed training dataset
  test_path: "./VAT"       # Path to preprocessed test dataset
  mean: [0.485, 0.456, 0.406]       # Normalization mean for images (ImageNet values)
  std: [0.229, 0.224, 0.225]        # Normalization std for images (ImageNet values)
  pre_train_path: "./gazefollow_extended"
  pre_test_path: "./gazefollow_extended"

train:
  batch_size: 24                    # Batch size for training
  epochs: 15                    # Number of training epochs
  lr: 0.001                         # 0.00001
  inout_lr: 0.001
  fuse_lr: 0.001
  optimizer: "Adam"                # Optimizer type (adam, adamw, sgd, etc.)
  weight_decay: 0.1                 # Weight decay for optimizer
  lr_scheduler:                     # Learning rate scheduler settings
    type: "cosine"                  # cosine / linear / warmup (for warmup training)
    step_size: 5                   # Step size for step scheduler / warmup: warmup_epochs
    gamma: 0.1                      # Decay factor for step/exponential scheduler
    min_lr: 1e-6                    # Minimum learning rate for cosine/other schedulers
  gradient_clipping: 5.0            # Gradient clipping threshold
  # GazeFollow pretrain params
  pre_lr: 0.01
  pre_batch_size: 60   # 60
  pre_epochs: 12   # 15
  pre_optimizer: "Adam"
  pre_lr_scheduler:
    type: "cosine"                  # cosine / linear
    step_size: 5                   # Step size for step scheduler
    gamma: 0.1                      # Decay factor for step/exponential scheduler
    min_lr: 1e-6                    # Minimum learning rate for cosine/other schedulers

eval:
  checkpoint: "gazelle_large_vitl14_inout.pt"  # Path to the checkpoint for evaluation
  batch_size: 24                    # Batch size for evaluation
  metrics:                          # Evaluation metrics
    - accuracy
    - precision
    - recall

inference:
  checkpoint: "gazelle_dinov2_vitl14_inout.pt"

logging:
  log_dir: "results/vat3_adam_bs24_bce_1e3/"   # "results/checkpoints20/"
  pre_dir: "results/prechecks70_adam_bs60_bce_1e2/"
  save_every: 5

hardware:
  device: "cuda:3"                    # Device to use (cuda, cpu)
  num_workers: 3                    # Number of data loader workers
  pin_memory: True                  # Pin memory for faster data loading
