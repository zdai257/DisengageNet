# General Configuration
model:
  name: "gazemoe_dinov2_vitl14_inout"  # "gazelle_dinov2_vitl14_inout"
  pretrained_path: "gazelle_dinov2_vitl14_inout.pt"  #"best_shared_epoch_4.pt"
  num_experts: 4  # Routed experts
  num_shared_experts: 2
  top_k: 2
  moe_type: "moe"  # vanilla / shared / else
  is_msf: 1
  bce_weight: 1.0
  mse_weight: 180.0
  angle_weight: 0.5
  vec_weight: 0.0
  pbce_loss: "mse"  #"mse" or "bce"
  reduction: "none"  #"mean" or "none"
  encoder:
    type: "DINOv2"                  # Pretrained image encoder (DINOv2, CLIP, etc.)
    pretrained: true                # Use pretrained weights
  decoder:
    hidden_size: 768                # Hidden size of the decoder
    depth: 12                       # Number of layers in the decoder
    num_heads: 8                    # Number of attention heads
    dropout: 0.1                    # Dropout rate

data:
  input_resolution: 448
  train_path: "./VAT"     # Path to preprocessed training dataset
  test_path: "./VAT"       # Path to preprocessed test dataset
  mean: [0.485, 0.456, 0.406]       # Normalization mean for images (ImageNet values)
  std: [0.229, 0.224, 0.225]        # Normalization std for images (ImageNet values)
  pre_train_path: "./gazefollow_extended"
  pre_test_path: "./gazefollow_extended"

train:
  batch_size: 32                    # Batch size for training
  epochs: 15                    # Number of training epochs
  lr: 0.001                         # 0.00001
  inout_lr: 0.01
  fuse_lr: 0.001
  layer_decay: 0                    # 0 or 1
  optimizer: "AdamW"                # Optimizer type (adam, adamw, sgd, etc.)
  weight_decay: 0.01                # Weight decay for optimizer
  lr_scheduler:                     # Learning rate scheduler settings
    type: "cosine"                  # cosine / linear / warmup (for warmup training)
    step_size: 5                   # Step size for step scheduler / warmup: warmup_epochs
    gamma: 0.1                      # Decay factor for step/exponential scheduler
    min_lr: 1e-6                    # Minimum learning rate for cosine/other schedulers
  gradient_clipping: 5.0            # Gradient clipping threshold
  # GazeFollow pretrain params
  pre_lr: 0.001
  pre_batch_size: 48   # 60
  pre_epochs: 15   # 15
  pre_optimizer: "Adam"
  pre_lr_scheduler:
    type: "cosine"                  # cosine / linear
    step_size: 5                   # Step size for step scheduler
    gamma: 0.1                      # Decay factor for step/exponential scheduler
    min_lr: 1e-6                    # Minimum learning rate for cosine/other schedulers

eval:
  checkpoint: "gazelle_large_vitl14_inout.pt"  # Path to the checkpoint for evaluation
  batch_size: 32                    # Batch size for evaluation
  metrics:                          # Evaluation metrics
    - accuracy
    - precision
    - recall

inference:
  checkpoint: "gazelle_dinov2_vitl14_inout.pt"

logging:
  log_dir: "results/moe/"   # auto naming
  pre_dir: "results/pretrainGF/"
  save_every: 5

hardware:
  device: "cuda:5"                    # Device to use (cuda, cpu)
  num_workers: 3                    # Number of data loader workers
  pin_memory: True                  # Pin memory for faster data loading
